#!/usr/bin/env python3
"""
Fix Duplicate Job IDs
Clean existing data and re-scrape with fixed job ID generation logic
"""

import os
import sys
import subprocess
from pathlib import Path

def clean_and_rescrape():
    """Clean existing data and re-scrape with fixed ID generation"""
    
    print("ğŸ”§ Fixing Duplicate Job IDs")
    print("=" * 50)
    
    # Check if scraped_data.json exists
    if os.path.exists('scraped_data.json'):
        print("ğŸ—‘ï¸  Removing existing scraped_data.json...")
        os.remove('scraped_data.json')
        print("âœ… Removed existing data")
    else:
        print("â„¹ï¸  No existing scraped_data.json found")
    
    # Check if there are any backup files
    backup_files = ['scraped_data_backup.json', 'scraped_data_clean.json']
    for backup_file in backup_files:
        if os.path.exists(backup_file):
            print(f"ğŸ—‘ï¸  Removing backup file: {backup_file}")
            os.remove(backup_file)
    
    print("\nğŸš€ Starting fresh scrape with fixed job ID generation...")
    print("ğŸ“‹ This will ensure all job IDs are globally unique")
    print("â° This may take several minutes...")
    
    try:
        # Run the multi-company spider
        print("\nğŸ”„ Running multi-company spider...")
        result = subprocess.run([
            sys.executable, 'run_multi_company_spider.py'
        ], capture_output=True, text=True)
        
        if result.returncode == 0:
            print("âœ… Scraping completed successfully!")
            print("ğŸ“Š New data generated with unique job IDs")
            
            # Verify the fix
            print("\nğŸ” Verifying job ID uniqueness...")
            verify_result = subprocess.run([
                sys.executable, 'check_duplicate_job_ids.py'
            ], capture_output=True, text=True)
            
            if verify_result.returncode == 0:
                print("âœ… Verification completed!")
                print("ğŸ‰ Job IDs should now be globally unique")
            else:
                print("âš ï¸  Verification failed, but scraping completed")
                
        else:
            print(f"âŒ Scraping failed with error code: {result.returncode}")
            print(f"Error output: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"âŒ Error during scraping: {e}")
        return False
    
    print("\nğŸ¯ Next Steps:")
    print("1. Test automation with a few jobs to verify the fix")
    print("2. Check that the correct company pages open")
    print("3. If issues persist, run check_duplicate_job_ids.py again")
    
    return True

def backup_existing_data():
    """Create a backup of existing data before cleaning"""
    
    if os.path.exists('scraped_data.json'):
        print("ğŸ’¾ Creating backup of existing data...")
        
        import json
        from datetime import datetime
        
        # Load existing data
        with open('scraped_data.json', 'r') as f:
            data = json.load(f)
        
        # Create backup filename with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_filename = f"scraped_data_backup_{timestamp}.json"
        
        # Save backup
        with open(backup_filename, 'w') as f:
            json.dump(data, f, indent=2)
        
        print(f"âœ… Backup created: {backup_filename}")
        return backup_filename
    
    return None

if __name__ == "__main__":
    print("ğŸ”§ Fix Duplicate Job IDs Tool")
    print("=" * 50)
    
    # Ask user if they want to backup existing data
    if os.path.exists('scraped_data.json'):
        response = input("\nâ“ Do you want to backup existing data before cleaning? (y/n): ")
        if response.lower() in ['y', 'yes']:
            backup_file = backup_existing_data()
            if backup_file:
                print(f"ğŸ’¾ Backup saved as: {backup_file}")
    
    # Confirm the action
    print("\nâš ï¸  This will:")
    print("   - Delete existing scraped_data.json")
    print("   - Re-scrape all companies with fixed job ID generation")
    print("   - Ensure all job IDs are globally unique")
    
    response = input("\nâ“ Continue with fixing duplicate job IDs? (y/n): ")
    
    if response.lower() in ['y', 'yes']:
        success = clean_and_rescrape()
        if success:
            print("\nğŸ‰ Duplicate job ID fix completed successfully!")
        else:
            print("\nâŒ Fix failed. Please check the error messages above.")
    else:
        print("\nâŒ Operation cancelled.") 